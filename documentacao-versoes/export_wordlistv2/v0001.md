# Documentação Técnica: export_wordlistv2.py

> Documentação gerada em domingo, 03 de novembro de 2024 às 05 horas e 17 minutos

## Visão Geral 

**export_wordlistv2.py** é o resultado da minha obsessão por organização e análise de dados.  Essa ferramenta, fruto de minhas noites em claro (e de muito café ☕),  extrai, processa e exporta listas de palavras de um banco de dados SQLite, gerando relatórios detalhados em JSON e YAML. É como se o *DataMiner* do filme *Minority Report* encontrasse um primo distante, mas especializado em lexicografia. 

Imagine um cenário onde você precisa analisar um *corpus* de texto, entender a frequência de palavras, a sua estrutura, identificar padrões e métricas.  **export_wordlistv2.py** é a sua solução, meu amigo.  É a *Matrix* revelando seus segredos, mas ao invés de código binário, são palavras e suas nuances.

## Estrutura e Componentes

O código é dividido em duas partes principais:

* **Classe `WordlistExporterV2`:** A alma do projeto, responsável por extrair, analisar e exportar as palavras. 
* **Função `main`:** A porta de entrada para o script, responsável por instanciar a classe e orquestrar o processo de exportação.

## Detalhes da Classe `WordlistExporterV2`

**`WordlistExporterV2`**  é uma classe que encapsula todas as funcionalidades do script. Seus métodos são como engrenagens de um relógio, trabalhando em perfeita sincronia para te entregar um resultado impecável.

### `__init__`

> *Sim, eu gosto de começar pelas bases!*

```python
    def __init__(self):
        self.console = Console()
```

O construtor da classe inicializa a instância da classe `Console` do pacote `rich`.  `Console` é a minha ferramenta de escolha para exibir informações na tela com um visual aprimorado,  como se o terminal fosse um palco pronto para uma apresentação.  

### `verificar_estrutura_banco`

> *Essa função é como uma lupa, examinando cada detalhe da estrutura do banco de dados. É preciso ter certeza de que a base está pronta para a extração!*

```python
    def verificar_estrutura_banco(self, cursor):
        """Verifica as colunas existentes na tabela"""
        cursor.execute("PRAGMA table_info(word_vectors)")
        colunas = {coluna[1] for coluna in cursor.fetchall()}
        return colunas
```

Essa função recebe um cursor do banco de dados e usa o comando `PRAGMA table_info` para verificar as colunas existentes na tabela `word_vectors`. Retorna um conjunto (`set`) com o nome das colunas. 

### `buscar_palavras_do_banco`

> *Um mergulho no oceano de dados, buscando as pérolas que formam a lista de palavras!*

```python
    def buscar_palavras_do_banco(self, cursor, colunas_existentes):
        """Busca palavras do banco adaptando-se às colunas existentes"""
        query_base = 'SELECT word, LENGTH(vector) as vector_size'
        
        if 'timestamp' in colunas_existentes:
            query = f"{query_base}, datetime(timestamp) as timestamp FROM word_vectors ORDER BY word"
        else:
            query = f"{query_base}, datetime('now') as timestamp FROM word_vectors ORDER BY word"
            
        cursor.execute(query)
        return cursor.fetchall()
```

Essa função, usando o poder do SQL, busca as palavras do banco de dados. Ela constrói uma query dinamicamente, adaptando-se à estrutura da tabela (se a coluna `timestamp` existe ou não). 

### `analisar_palavras`

> *Hora de desvendar os mistérios da linguagem! É aqui que a magia acontece. A análise detalhada das palavras revela suas características e padrões ocultos.*

```python
    def analisar_palavras(self, palavras: List[str]) -> Dict:
        """Análise detalhada das palavras"""
        vogais = set('aeiouáéíóúâêîôûãõàèìòùäëïöü')
        
        stats = {
            'metricas_basicas': {
                'total_palavras': len(palavras),
                'palavras_unicas': len(set(palavras)),
                'comprimento_medio': round(statistics.mean(len(p) for p in palavras), 2),
                'comprimento_mediano': statistics.median(len(p) for p in palavras),
                'desvio_padrao_comprimento': round(statistics.stdev(len(p) for p in palavras), 2) if len(palavras) > 1 else 0,
            },
            'analise_caracteres': {
                'distribuicao_primeira_letra': dict(Counter(p[0].lower() for p in palavras).most_common(10)),
                'chars_especiais': dict(Counter(c for p in palavras for c in p if not c.isalnum()).most_common(10)),
                'vogais_mais_comuns': dict(Counter(c for p in palavras for c in p.lower() if c in vogais).most_common(10)),
                'consoantes_mais_comuns': dict(Counter(c for p in palavras for c in p.lower() if c.isalpha() and c not in vogais).most_common(10))
            },
            'distribuicao_tamanhos': dict(Counter(len(p) for p in palavras)),
            'padroes_linguisticos': {
                'prefixos_comuns': dict(Counter(p[:3] for p in palavras if len(p) > 3).most_common(10)),
                'sufixos_comuns': dict(Counter(p[-3:] for p in palavras if len(p) > 3).most_common(10)),
            },
            'indicadores_estatisticos': {
                'densidade_lexica': round(len(set(palavras)) / len(palavras), 4),
                'palavras_por_tamanho': self.agrupar_por_tamanho(palavras),
                'distribuicao_acentuacao': self.analisar_acentuacao(palavras),
            }
        }
        return stats
```

Essa função é um verdadeiro centro de pesquisa da linguagem. Ela recebe uma lista de palavras e realiza uma série de análises:

* **Métricas básicas:** Quantidade total de palavras, número de palavras únicas, comprimento médio, comprimento mediano e desvio padrão do comprimento das palavras.
* **Análise de caracteres:** Distribuição da primeira letra das palavras, caracteres especiais mais comuns, vogais mais comuns e consoantes mais comuns.
* **Distribuição de tamanhos:** Agrupa as palavras por tamanho, mostrando a quantidade de palavras com cada comprimento.
* **Padrões linguísticos:** Identifica os prefixos e sufixos mais comuns.
* **Indicadores estatísticos:** Calcula a densidade léxica (relação entre o número de palavras únicas e o número total de palavras), agrupa as palavras por tamanho e analisa a distribuição de acentuação.

### `agrupar_por_tamanho`

> *A organização é a chave do sucesso! Essa função coloca cada palavra no seu devido lugar, agrupando-as por tamanho.*

```python
    def agrupar_por_tamanho(self, palavras: List[str]) -> Dict[int, int]:
        """Agrupa palavras por tamanho"""
        return dict(sorted(Counter(len(p) for p in palavras).items()))
```

Essa função recebe uma lista de palavras e retorna um dicionário com a contagem das palavras agrupadas por tamanho.

### `analisar_acentuacao`

> *Acentos são como as nuances de um poema, dando ritmo e melodia à linguagem. Essa função analisa a presença de acentos nas palavras.*

```python
    def analisar_acentuacao(self, palavras: List[str]) -> Dict[str, int]:
        """Analisa padrões de acentuação"""
        acentos = 'áéíóúâêîôûãõàèìòùäëïöü'
        return {
            'palavras_com_acento': sum(1 for p in palavras if any(c in p for c in acentos)),
            'palavras_sem_acento': sum(1 for p in palavras if not any(c in p for c in acentos))
        }
```

Essa função recebe uma lista de palavras e retorna um dicionário com a quantidade de palavras com e sem acentos.

### `exportar_palavras`

> *O momento da verdade! A função `exportar_palavras` transforma a análise em arquivos concretos, prontos para serem compartilhados e explorados.*

```python
    def exportar_palavras(self):
        """Exporta palavras e análises para JSON e YAML"""
        output_dir = Path('vector-exported-data')
        output_dir.mkdir(exist_ok=True)
        
        try:
            with sqlite3.connect('vetor-words-database-index.db') as conn:
                cursor = conn.cursor()
                
                # Verifica estrutura do banco
                colunas_existentes = self.verificar_estrutura_banco(cursor)
                
                # Busca dados adaptando-se às colunas existentes
                resultados = self.buscar_palavras_do_banco(cursor, colunas_existentes)
                
                palavras = [row[0] for row in resultados]
                
                # Realizar análises
                analytics = self.analisar_palavras(palavras)
                
                # Preparar dados para exportação
                data = {
                    'metadata': {
                        'timestamp_export': datetime.now().isoformat(),
                        'total_palavras': len(palavras),
                        'versao_exportador': '2.0.1',
                        'banco_origem': 'ultra-speed-words.db',
                        'colunas_disponiveis': list(colunas_existentes)
                    },
                    'analytics': analytics,
                    'palavras': sorted(palavras)  # Lista ordenada de palavras
                }
                
                # Gerar hash e nomes de arquivo
                hash_arquivo = hashlib.sha256(json.dumps(data, ensure_ascii=False).encode()).hexdigest()[:8]
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                base_filename = f'wordlist_v2_{timestamp}_{hash_arquivo}'
                
                # Exportar JSON
                json_path = output_dir / f'{base_filename}.json'
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                
                # Exportar YAML
                yaml_path = output_dir / f'{base_filename}.yaml'
                with open(yaml_path, 'w', encoding='utf-8') as f:
                    yaml.dump(data, f, allow_unicode=True, sort_keys=False)
                
                # Exibir relatório
                self.exibir_relatorio(data, base_filename)
                
        except Exception as e:
            self.console.print(f"[red]Erro durante a exportação: {str(e)}[/]")
            raise
```

Essa função, a cereja do bolo, é responsável por realizar a exportação dos dados para JSON e YAML.  Seu funcionamento é bem detalhado e demonstra minha paixão por código limpo e organizado:

1. **Cria o diretório de saída:** Define o caminho para o diretório `vector-exported-data` e cria a pasta caso ela não exista.
2. **Conecta ao banco de dados:** Estabelece a conexão com o banco de dados `vetor-words-database-index.db` usando SQLite.
3. **Verifica a estrutura do banco de dados:** Chama a função `verificar_estrutura_banco` para obter as colunas da tabela `word_vectors`.
4. **Busca as palavras:** Chama a função `buscar_palavras_do_banco` para extrair as palavras do banco de dados, adaptando-se à estrutura da tabela.
5. **Realiza a análise das palavras:** Chama a função `analisar_palavras` para realizar a análise detalhada das palavras.
6. **Prepara os dados para exportação:** Organiza os dados em um dicionário, incluindo metadados, resultados da análise e a lista de palavras ordenada.
7. **Gera o hash e nomes de arquivo:** Calcula o hash do conteúdo do arquivo para garantir a integridade dos dados, usando o algoritmo SHA-256.
8. **Exporta para JSON:** Cria o arquivo JSON com os dados, usando o módulo `json` e formatando a saída de forma organizada.
9. **Exporta para YAML:** Cria o arquivo YAML com os dados, usando o módulo `yaml`, preservando a estrutura original dos dados e garantindo legibilidade.
10. **Exibe o relatório:** Chama a função `exibir_relatorio` para exibir um relatório detalhado dos resultados da análise.


### `exibir_relatorio`

> *Um relatório completo, como um relatório médico detalhado, mas com foco em dados e análise de palavras. É hora de apresentar os resultados da pesquisa linguística!*

```python
    def exibir_relatorio(self, data: Dict, filename: str):
        """Exibe relatório detalhado com indicadores"""
        self.console.print("\n[bold green]✨ Exportação Concluída com Sucesso![/]\n")
        
        # Tabela principal
        table = Table(title="📊 Resumo da Exportação")
        table.add_column("Métrica", style="cyan", justify="right")
        table.add_column("Valor", style="magenta")
        
        # Métricas básicas
        for key, value in data['analytics']['metricas_basicas'].items():
            table.add_row(
                key.replace('_', ' ').title(),
                f"{value:.2f}" if isinstance(value, float) else str(value)
            )
        
        self.console.print(table)
        
        # Painéis de análise
        panels = []
        
        # Distribuição de tamanhos
        dist_table = Table(title="📏 Distribuição de Tamanhos")
        dist_table.add_column("Tamanho", justify="center")
        dist_table.add_column("Quantidade", justify="right")
        for size, count in sorted(data['analytics']['distribuicao_tamanhos'].items())[:10]:
            dist_table.add_row(str(size), str(count))
        panels.append(Panel(dist_table, title="Distribuição"))
        
        # Caracteres mais comuns
        chars_table = Table(title="🔤 Caracteres Mais Comuns")
        chars_table.add_column("Tipo", justify="right")
        chars_table.add_column("Top 5", justify="left")
        for char_type, chars in data['analytics']['analise_caracteres'].items():
            chars_str = ", ".join(f"{c}({n})" for c, n in list(chars.items())[:5])
            chars_table.add_row(char_type.replace('_', ' ').title(), chars_str)
        panels.append(Panel(chars_table, title="Caracteres"))
        
        # Exibir painéis
        self.console.print(Columns(panels))
        
        # Informações do arquivo
        self.console.print(f"\n[blue]📁 Arquivos gerados:[/]")
        self.console.print(f"  • JSON: {filename}.json")
        self.console.print(f"  • YAML: {filename}.yaml")
        self.console.print(f"[blue]🔑 Hash do conteúdo:[/] {filename.split('_')[-1]}")
        self.console.print(f"\n[green]✅ Total de palavras exportadas:[/] {data['metadata']['total_palavras']}")
```

Essa função, como um maestro, organiza a apresentação dos resultados da análise,  usando as funcionalidades do pacote `rich` para criar uma interface de usuário rica e informativa.  Ela apresenta um resumo da exportação em uma tabela,  exibe gráficos e informações sobre a distribuição de tamanhos e caracteres, e fornece detalhes sobre os arquivos gerados e o hash do conteúdo. 

## Fluxo de Execução Principal

A função `main`, a porta de entrada para o script, é bem simples, mas poderosa:

```python
if __name__ == '__main__':
    console = Console()
    console.print("[yellow]🚀 Iniciando exportação da lista de palavras...[/]")
    
    try:
        exporter = WordlistExporterV2()
        exporter.exportar_palavras()
    except Exception as e:
        console.print(f"[red]❌ Erro fatal: {str(e)}[/]")
```

Ela cria uma instância da classe `Console` do pacote `rich`, exibe uma mensagem de início da exportação, instância a classe `WordlistExporterV2` e chama o método `exportar_palavras`.  Em caso de erro, exibe uma mensagem de erro na cor vermelha.

## Dependências e Requisitos

O script `export_wordlistv2.py` depende dos seguintes pacotes Python:

* **sqlite3:** Para interagir com o banco de dados SQLite.
* **json:** Para serializar os dados em formato JSON.
* **yaml:** Para serializar os dados em formato YAML.
* **hashlib:** Para calcular o hash do conteúdo do arquivo.
* **numpy:** Para manipular dados numéricos (embora não seja usado diretamente nesse script).
* **pandas:** Para manipular dados em formato tabular (embora não seja usado diretamente nesse script).
* **datetime:** Para trabalhar com datas e horários.
* **pathlib:** Para manipular caminhos de arquivos.
* **rich:** Para criar uma interface de usuário rica e informativa.
* **collections:** Para usar a classe `Counter` para contar a frequência de elementos.
* **statistics:** Para calcular estatísticas como média, mediana e desvio padrão.
* **dataclasses:** Para definir classes de dados de forma concisa e organizada (embora não seja usado diretamente nesse script).

## Exemplos de Uso

Para usar o script `export_wordlistv2.py`, você precisa ter um banco de dados SQLite com uma tabela chamada `word_vectors`, contendo pelo menos as colunas `word` e `vector`.

```bash
# Executar o script
python export_wordlistv2.py 
```

## Considerações Técnicas Importantes

* **Tratamento de Erros:**  O script inclui tratamento de erros básico, mas pode ser aprimorado para lidar com casos específicos.
* **Organização do Código:**  O código é organizado em classes e métodos, facilitando a manutenção e reutilização de código.
* **Documentação:**  O script é bem documentado,  com docstrings detalhadas para cada método, facilitando a compreensão do código.
* **Uso de Pacotes:**  O script utiliza pacotes Python de alto nível, como `rich`,  `yaml` e `json`,  o que aumenta sua eficiência e produtividade.

## Possíveis Melhorias e Recomendações

* **Melhorias no Tratamento de Erros:**  Implementar tratamento de erros mais robusto para lidar com diferentes cenários.
* **Adição de Funcionalidades:**  Criar novas funções de análise, como identificar sinônimos, analisar a estrutura gramatical das palavras ou realizar análise semântica.
* **Integração com outros Ferramentas:**  Integrar o script com outras ferramentas de análise de dados, como ferramentas de visualização ou de processamento de linguagem natural.
* **Teste Automatizado:**  Criar testes automatizados para garantir a qualidade e a corretude do código.


## Análise de Segurança e Performance

* **Segurança:**  O script não apresenta vulnerabilidades de segurança relevantes, pois não lida com dados sensíveis ou acesso a recursos externos.
* **Performance:**  O script é relativamente rápido, principalmente devido ao uso de SQLite e do pacote `rich` para otimizar a interface do usuário.  No entanto, a performance pode ser melhorada com a utilização de técnicas de otimização para processamento de dados, como o uso de bibliotecas como `numpy` para operações matemáticas mais eficientes.

## Conclusão

**export_wordlistv2.py** é um script Python versátil e eficiente para extrair, analisar e exportar listas de palavras de um banco de dados SQLite,  gerando relatórios detalhados.  É a solução perfeita para  quem precisa de um sistema confiável e completo para lidar com análise de dados linguísticos.

## Contato

> Se você está pronto para embarcar nessa jornada de descobertas linguísticas, entre em contato comigo! 🤝

[![Linkedin](https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/itilmgf/)
[![Gmail](https://img.shields.io/badge/-Gmail-c14436?style=flat-square&logo=gmail&logoColor=white)](mailto:oeliasandrade@gmail.com)
[![Whatsapp](https://img.shields.io/badge/-Whatsapp-25D366?style=flat-square&logo=whatsapp&logoColor=white)](https://api.whatsapp.com/send?phone=5544988597116)

## Repositórios

> Meus repositórios estão cheios de projetos que podem te inspirar! 💻

[![Github](https://img.shields.io/badge/-GitHub-black?style=flat-square&logo=github&logoColor=white)](https://github.com/chaos4455)
[![Github](https://img.shields.io/badge/-GitHub-black?style=flat-square&logo=github&logoColor=white)](https://github.com/evolucaoit)
[![Github](https://img.shields.io/badge/-GitHub-black?style=flat-square&logo=github&logoColor=white)](https://github.com/replika-ai-solutions)

> *"Eu sou o que eu sou, porque eu sou o que eu escolhi ser. Não sou um produto do meu passado,  e meu futuro está nas minhas mãos."*  - **O Guia do Mochileiro das Galáxias**
